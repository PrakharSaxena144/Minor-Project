{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "PXqSquru_2Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layouts: https://github.com/PrakharSaxena144/Minor-Project/blob/main/input%20data%20layouts_5.pdf\n",
        "\n",
        "Dataset for Unet: https://drive.google.com/drive/folders/1bCpjt5gj1pcKABma2espjTPzSGuPFwtA?usp=drive_link"
      ],
      "metadata": {
        "id": "-Xnr7K7al75Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Codes"
      ],
      "metadata": {
        "id": "EtIxZQ2pCuno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Rotator"
      ],
      "metadata": {
        "id": "xK0wn7zZC38i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def rotate_image_pil(image_path, angle, output_path=\"rotated_image.png\"):\n",
        "    img = Image.open(image_path)\n",
        "    rotated = img.rotate(angle, expand=True)  # expand keeps full image\n",
        "    rotated.save(output_path)\n",
        "    print(f\"Rotated image saved at: {output_path}\")\n",
        "\n",
        "# Example\n",
        "rotate_image_pil(\"/content/edges.png\", -40, \"rotated_cw.png\")\n",
        "\n",
        "# With angle = 90 â†’ rotates counterclockwise\n",
        "# With angle = -90 â†’ rotates clockwise"
      ],
      "metadata": {
        "id": "weCiwcXoC0LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selectable Text Remover"
      ],
      "metadata": {
        "id": "T4DqWKOsCoEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_PDF = '/content/input data layouts_5.pdf'\n",
        "OUTPUT_PDF = 'map_no_selectable_text.pdf'\n",
        "\n",
        "# --- Main Script ---\n",
        "\n",
        "def remove_selectable_text(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Opens a PDF, finds all selectable text on each page, redacts it,\n",
        "    and saves the result to a new PDF.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"Error: Input file not found at '{input_path}'\")\n",
        "        return\n",
        "\n",
        "    print(f\"Opening '{input_path}'...\")\n",
        "    # Open the PDF\n",
        "    doc = fitz.open(input_path)\n",
        "\n",
        "    # Iterate through each page of the document\n",
        "    for page_num, page in enumerate(doc):\n",
        "        print(f\"  - Processing page {page_num + 1}/{len(doc)}...\")\n",
        "\n",
        "        # 1. Find all text \"words\" on the page.\n",
        "        # This returns a list of items, where each item has the\n",
        "        # coordinates (x0, y0, x1, y1) and the text itself.\n",
        "        words = page.get_text(\"words\")\n",
        "\n",
        "        # 2. Add a redaction annotation for each word's bounding box.\n",
        "        # A redaction securely removes content from the specified area.\n",
        "        for word in words:\n",
        "            # The first 4 elements of the 'word' item are its coordinates\n",
        "            bbox = fitz.Rect(word[:4])\n",
        "            page.add_redact_annot(bbox)\n",
        "\n",
        "        # 3. Apply all the redactions on the page.\n",
        "        # This step permanently removes the content.\n",
        "        page.apply_redactions()\n",
        "\n",
        "    # Save the modified document to the output path\n",
        "    # 'garbage=4' cleans up unused objects, 'deflate=True' compresses the file\n",
        "    print(f\"Saving cleaned PDF to '{output_path}'...\")\n",
        "    doc.save(output_path, garbage=4, deflate=True)\n",
        "    doc.close()\n",
        "\n",
        "    print(\"Done! All selectable text has been removed. âœ¨\")\n",
        "\n",
        "# Run the main function\n",
        "remove_selectable_text(INPUT_PDF, OUTPUT_PDF)"
      ],
      "metadata": {
        "id": "cxCw3wJ1Cmos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWFE-OaBCmlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSIM\n",
        "## (Structural Similarity Index Measure)"
      ],
      "metadata": {
        "id": "zn8UvLnFEt5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used for comparing 2 images.\n",
        "\n",
        "Things it do:\n",
        "\n",
        "Tiling,Tiling comparison, Marking as red (Greater than a certain threshold), Changed Area Overlay\n",
        "\n",
        "Link:\n",
        "https://colab.research.google.com/drive/1uxKlL402nxPk1Se2jkFoEcyIM0MMzW6g#scrollTo=EYfy32RxDGx-"
      ],
      "metadata": {
        "id": "GCgasmBEEt18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Without Overlay (Overlay means showing regions that are marked as different)"
      ],
      "metadata": {
        "id": "Xdnbb-ESg-1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tile-based hybrid SSIM + intensity change detection\n",
        "# Red-transparent overlay on the actual satellite image (presentation-quality)\n",
        "#\n",
        "# Requirements:\n",
        "# pip install opencv-python scikit-image matplotlib numpy Pillow\n",
        "\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "\n",
        "# -----------------------------\n",
        "# User-configurable parameters\n",
        "# -----------------------------\n",
        "SATELLITE_PATH = \"/content/color_segmented.png\"      # actual satellite image (RGB/BGR)\n",
        "LAYOUT_PATH    = \"/content/grayscale_edges/edges_page_7.png\"  # planned layout image\n",
        "TILE_SIZE      = 32      # user tile size (changeable)\n",
        "HYBRID_THRESH  = 0.3      # medium sensitivity\n",
        "SSIM_WEIGHT    = 0.5      # weight for (1 - ssim)\n",
        "INT_WEIGHT     = 0.5      # weight for normalized intensity diff\n",
        "OUTPUT_DIR     = \"/content/output_results\"\n",
        "SAVE_TILE_COMPARISONS = True  # save every tile side-by-side\n",
        "MAX_SAMPLE_TILES_DISPLAY = 8  # how many tile comparisons to show inline\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def ensure_dir(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def load_image_bgr(path):\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {path}\")\n",
        "    return img\n",
        "\n",
        "def pad_to_multiple(img, tile_size, pad_color=(0,0,0)):\n",
        "    h, w = img.shape[:2]\n",
        "    pad_h = (math.ceil(h / tile_size) * tile_size) - h\n",
        "    pad_w = (math.ceil(w / tile_size) * tile_size) - w\n",
        "    if pad_h == 0 and pad_w == 0:\n",
        "        return img, (0,0)\n",
        "    # bottom and right padding\n",
        "    padded = cv2.copyMakeBorder(img, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)\n",
        "    return padded, (pad_h, pad_w)\n",
        "\n",
        "def create_tile_coords(img_shape, tile_size):\n",
        "    h, w = img_shape[:2]\n",
        "    coords = []\n",
        "    for y in range(0, h, tile_size):\n",
        "        for x in range(0, w, tile_size):\n",
        "            coords.append((x, y))\n",
        "    return coords\n",
        "\n",
        "def compute_tile_stats(tileA, tileB):\n",
        "    # Convert to gray\n",
        "    tAg = cv2.cvtColor(tileA, cv2.COLOR_BGR2GRAY) if tileA.ndim == 3 else tileA.copy()\n",
        "    tBg = cv2.cvtColor(tileB, cv2.COLOR_BGR2GRAY) if tileB.ndim == 3 else tileB.copy()\n",
        "\n",
        "    # Ensure same size\n",
        "    if tAg.shape != tBg.shape:\n",
        "        tBg = cv2.resize(tBg, (tAg.shape[1], tAg.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # SSIM: choose win_size <= min(dim) and odd and >=3 (skimage requires >=3, default 7)\n",
        "    min_side = min(tAg.shape[0], tAg.shape[1])\n",
        "    win_size = min(7, min_side)\n",
        "    if win_size < 3:\n",
        "        # fallback: if extremely small (shouldn't happen because we pad), just return neutral\n",
        "        ssim_val = 1.0\n",
        "    else:\n",
        "        if win_size % 2 == 0:\n",
        "            win_size -= 1\n",
        "        try:\n",
        "            ssim_val = ssim(tAg, tBg, win_size=win_size)\n",
        "        except Exception:\n",
        "            ssim_val = ssim(tAg, tBg, data_range=tAg.max()-tAg.min() if tAg.max()!=tAg.min() else 1)\n",
        "\n",
        "    # Intensity difference normalized [0,1]\n",
        "    absdiff = np.abs(tAg.astype(np.float32) - tBg.astype(np.float32))\n",
        "    mean_diff = absdiff.mean()  # 0..255\n",
        "    norm_diff = mean_diff / 255.0\n",
        "\n",
        "    return ssim_val, norm_diff\n",
        "\n",
        "# -----------------------------\n",
        "# Main pipeline\n",
        "# -----------------------------\n",
        "def run_tile_change_detection(sat_path, layout_path, tile_size=TILE_SIZE,\n",
        "                              hybrid_thresh=HYBRID_THRESH,\n",
        "                              ssim_w=SSIM_WEIGHT, int_w=INT_WEIGHT,\n",
        "                              out_dir=OUTPUT_DIR, save_tile_comparisons=SAVE_TILE_COMPARISONS):\n",
        "    ensure_dir(out_dir)\n",
        "    ensure_dir(os.path.join(out_dir, \"tiles_comparisons\"))\n",
        "\n",
        "    # Step 1: Load images\n",
        "    sat = load_image_bgr(sat_path)\n",
        "    layout = load_image_bgr(layout_path)\n",
        "    print(f\"Loaded sat: {sat.shape}, layout: {layout.shape}\")\n",
        "\n",
        "    # Step 2: Align by resizing layout to satellite shape (preserve sat resolution)\n",
        "    layout_resized = cv2.resize(layout, (sat.shape[1], sat.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "    print(f\"Aligned layout to satellite size: {sat.shape}\")\n",
        "\n",
        "    # Save intermediate step visualizations\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step1_satellite.png\"), sat)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step1_layout_aligned.png\"), layout_resized)\n",
        "    print(\"Saved: step1_satellite.png, step1_layout_aligned.png\")\n",
        "\n",
        "    # Step 3: Pad images so dimensions divisible by tile_size (avoid tiny edge tiles)\n",
        "    sat_padded, (pad_h, pad_w) = pad_to_multiple(sat, tile_size)\n",
        "    layout_padded, _ = pad_to_multiple(layout_resized, tile_size)\n",
        "    print(f\"Padded images -> new shape: {sat_padded.shape} (pad_h={pad_h}, pad_w={pad_w})\")\n",
        "\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step2_satellite_padded.png\"), sat_padded)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step2_layout_padded.png\"), layout_padded)\n",
        "    print(\"Saved: step2_satellite_padded.png, step2_layout_padded.png\")\n",
        "\n",
        "    # Step 4: Iterate tiles and compute hybrid change score\n",
        "    coords = create_tile_coords(sat_padded.shape, tile_size)\n",
        "    heat_values = np.zeros((sat_padded.shape[0] // tile_size, sat_padded.shape[1] // tile_size), dtype=np.float32)\n",
        "    change_mask_tiles = np.zeros_like(heat_values, dtype=np.uint8)\n",
        "\n",
        "    comparisons_for_display = []  # store small set for display\n",
        "\n",
        "    for idx, (x, y) in enumerate(coords):\n",
        "        tx = x; ty = y\n",
        "        tileA = sat_padded[ty:ty+tile_size, tx:tx+tile_size]\n",
        "        tileB = layout_padded[ty:ty+tile_size, tx:tx+tile_size]\n",
        "\n",
        "        ssim_val, norm_diff = compute_tile_stats(tileA, tileB)\n",
        "\n",
        "        # hybrid score: higher => more change\n",
        "        hybrid_score = ssim_w * (1.0 - ssim_val) + int_w * norm_diff\n",
        "        # store into heat array\n",
        "        row = ty // tile_size\n",
        "        col = tx // tile_size\n",
        "        heat_values[row, col] = hybrid_score\n",
        "\n",
        "        # mark changed tile if hybrid_score > threshold\n",
        "        if hybrid_score > hybrid_thresh:\n",
        "            change_mask_tiles[row, col] = 1\n",
        "        else:\n",
        "            change_mask_tiles[row, col] = 0\n",
        "\n",
        "        # Save side-by-side comparison image for this tile (sat | layout | diff visualization)\n",
        "        if save_tile_comparisons:\n",
        "            # create visualization: sat (left), layout (mid), absdiff (right)\n",
        "            tA_rgb = tileA.copy()\n",
        "            tB_rgb = tileB.copy()\n",
        "            tA_gray = cv2.cvtColor(tA_rgb, cv2.COLOR_BGR2GRAY)\n",
        "            tB_gray = cv2.cvtColor(tB_rgb, cv2.COLOR_BGR2GRAY)\n",
        "            absd = cv2.absdiff(tA_gray, tB_gray)\n",
        "            # scale absd for display\n",
        "            absd_vis = cv2.cvtColor(absd, cv2.COLOR_GRAY2BGR)\n",
        "            # add a small colored overlay for hybrid score (red intensity)\n",
        "            red_overlay = np.zeros_like(tA_rgb)\n",
        "            red_int = int(np.clip(hybrid_score, 0, 1) * 255)\n",
        "            red_overlay[:, :, 2] = red_int\n",
        "            # blend overlay with actual tile for the rightmost small preview (optional)\n",
        "            blended = cv2.addWeighted(tA_rgb, 0.7, red_overlay, 0.3, 0)\n",
        "\n",
        "            comparison = np.hstack([tA_rgb, tB_rgb, absd_vis, blended])\n",
        "            fname = os.path.join(out_dir, \"tiles_comparisons\", f\"tile_{row}_{col}_hyb{hybrid_score:.3f}.png\")\n",
        "            cv2.imwrite(fname, comparison)\n",
        "\n",
        "            # keep a small set for inline display\n",
        "            if len(comparisons_for_display) < MAX_SAMPLE_TILES_DISPLAY:\n",
        "                comparisons_for_display.append((fname, hybrid_score, (row, col), ssim_val, norm_diff))\n",
        "\n",
        "    # Step 5: Build the pixel-level change heatmap (upsample tile heat to image size)\n",
        "    tile_rows, tile_cols = heat_values.shape\n",
        "    heatmap = cv2.resize(heat_values, (sat_padded.shape[1], sat_padded.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Normalize heatmap 0..1\n",
        "    heatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "\n",
        "    # Create red overlay where heatmap intensity > 0 (we'll apply transparency)\n",
        "    overlay = sat_padded.copy().astype(np.float32)\n",
        "    red_layer = np.zeros_like(overlay)\n",
        "    red_layer[:, :, 2] = (heatmap_norm * 255).astype(np.uint8)  # red channel intensity proportional to heat\n",
        "\n",
        "    # binary mask of significant change by tile mask (expand tiles)\n",
        "    change_mask_pixels = cv2.resize(change_mask_tiles.astype(np.uint8),\n",
        "                                    (sat_padded.shape[1], sat_padded.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Create red-transparent overlay: alpha = heatmap_norm * change_mask_pixels\n",
        "    alpha = (heatmap_norm * change_mask_pixels).astype(np.float32)  # 0..1\n",
        "    alpha_3 = np.repeat(alpha[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "    overlay_out = (overlay * (1 - alpha_3) + red_layer.astype(np.float32) * (alpha_3)).astype(np.uint8)\n",
        "\n",
        "    # Crop back to original size (remove padding) for saving/display\n",
        "    h0, w0 = sat.shape[:2]\n",
        "    overlay_cropped = overlay_out[0:h0, 0:w0]\n",
        "    heatmap_cropped = (heatmap_norm * 255).astype(np.uint8)[0:h0, 0:w0]\n",
        "    change_mask_cropped = change_mask_pixels[0:h0, 0:w0] * 255\n",
        "\n",
        "    # Step 6: Save and display results after major steps\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step3_tile_heat_raw.png\"), (heatmap_norm * 255).astype(np.uint8))\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step3_tile_binary_mask.png\"), change_mask_cropped)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"final_overlay_on_satellite.png\"), overlay_cropped)\n",
        "\n",
        "    print(f\"\\nSaved outputs to: {out_dir}\")\n",
        "    print(\" - step3_tile_heat_raw.png (tile intensity heatmap, 0..255)\")\n",
        "    print(\" - step3_tile_binary_mask.png (binary changed tiles mask)\")\n",
        "    print(\" - final_overlay_on_satellite.png (satellite with red-transparent overlay)\")\n",
        "\n",
        "    # Inline displays\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(cv2.cvtColor(sat, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Satellite\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(cv2.cvtColor(layout_resized, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Aligned Layout\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(cv2.cvtColor(overlay_cropped, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Overlay (Red = Deviation)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Show heatmap separately (red intensity)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(heatmap_cropped, cmap=\"hot\")\n",
        "    plt.title(\"Tile Hybrid Heatmap (hot)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "    # Show some tile comparisons\n",
        "    if comparisons_for_display:\n",
        "        print(\"\\nSample tile comparisons saved (sat | layout | absdiff | blended overlay):\")\n",
        "        for fname, hybrid_score, (r,c), ssim_val, norm_diff in comparisons_for_display:\n",
        "            print(f\" Tile ({r},{c}): hybrid={hybrid_score:.3f}, ssim={ssim_val:.3f}, int_diff={norm_diff:.3f}  -> {fname}\")\n",
        "            comp = Image.open(fname)\n",
        "            display(comp)  # in notebooks this will show the image\n",
        "    else:\n",
        "        print(\"No tile comparisons to display.\")\n",
        "\n",
        "    # Summary stats\n",
        "    total_tiles = tile_rows * tile_cols\n",
        "    changed_tiles = int(change_mask_tiles.sum())\n",
        "    percent_changed_tiles = (changed_tiles / total_tiles) * 100.0\n",
        "    mean_hybrid = float(heat_values.mean())\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"Tile size: {tile_size} px\")\n",
        "    print(f\"Tiles: {total_tiles} (rows={tile_rows}, cols={tile_cols})\")\n",
        "    print(f\"Changed tiles: {changed_tiles}\")\n",
        "    print(f\"Change coverage (tiles): {percent_changed_tiles:.2f}%\")\n",
        "    print(f\"Mean hybrid score: {mean_hybrid:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"heat_values\": heat_values,\n",
        "        \"change_mask_tiles\": change_mask_tiles,\n",
        "        \"overlay_path\": os.path.join(out_dir, \"final_overlay_on_satellite.png\"),\n",
        "        \"heatmap_path\": os.path.join(out_dir, \"step3_tile_heat_raw.png\"),\n",
        "        \"tile_comparisons_dir\": os.path.join(out_dir, \"tiles_comparisons\")\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Run (interactive tile size)\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "    try:\n",
        "        ts_input = input(f\"Enter tile size (default {TILE_SIZE}): \").strip()\n",
        "        if ts_input != \"\":\n",
        "            TILE_SIZE = int(ts_input)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Use medium sensitivity (threshold=0.3) as requested\n",
        "    res = run_tile_change_detection(SATELLITE_PATH, LAYOUT_PATH, tile_size=TILE_SIZE,\n",
        "                                    hybrid_thresh=HYBRID_THRESH)\n"
      ],
      "metadata": {
        "id": "PFpC7ARafahz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Overlay"
      ],
      "metadata": {
        "id": "pXZnF7w8hjW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tile-based hybrid SSIM + intensity change detection\n",
        "# Red-transparent overlay on the actual satellite image (presentation-quality)\n",
        "#\n",
        "# Requirements:\n",
        "# pip install opencv-python scikit-image matplotlib numpy Pillow\n",
        "\n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from PIL import Image\n",
        "\n",
        "# -----------------------------\n",
        "# User-configurable parameters\n",
        "# -----------------------------\n",
        "SATELLITE_PATH = \"/content/color_segmented.png\"      # actual satellite image (RGB/BGR)\n",
        "LAYOUT_PATH    = \"/content/color_segmented/segmented_page_3.png\"  # planned layout image\n",
        "TILE_SIZE      = 32      # user tile size (changeable)\n",
        "HYBRID_THRESH  = 0.6      # medium sensitivity\n",
        "SSIM_WEIGHT    = 0.5      # weight for (1 - ssim)\n",
        "INT_WEIGHT     = 0.5      # weight for normalized intensity diff\n",
        "OUTPUT_DIR     = \"/content/output_results\"\n",
        "SAVE_TILE_COMPARISONS = True  # save every tile side-by-side\n",
        "MAX_SAMPLE_TILES_DISPLAY = 8  # how many tile comparisons to show inline\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def ensure_dir(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def load_image_bgr(path):\n",
        "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Could not read image: {path}\")\n",
        "    return img\n",
        "\n",
        "def pad_to_multiple(img, tile_size, pad_color=(0,0,0)):\n",
        "    h, w = img.shape[:2]\n",
        "    pad_h = (math.ceil(h / tile_size) * tile_size) - h\n",
        "    pad_w = (math.ceil(w / tile_size) * tile_size) - w\n",
        "    if pad_h == 0 and pad_w == 0:\n",
        "        return img, (0,0)\n",
        "    # bottom and right padding\n",
        "    padded = cv2.copyMakeBorder(img, 0, pad_h, 0, pad_w, cv2.BORDER_REFLECT)\n",
        "    return padded, (pad_h, pad_w)\n",
        "\n",
        "def create_tile_coords(img_shape, tile_size):\n",
        "    h, w = img_shape[:2]\n",
        "    coords = []\n",
        "    for y in range(0, h, tile_size):\n",
        "        for x in range(0, w, tile_size):\n",
        "            coords.append((x, y))\n",
        "    return coords\n",
        "\n",
        "def compute_tile_stats(tileA, tileB):\n",
        "    # Convert to gray\n",
        "    tAg = cv2.cvtColor(tileA, cv2.COLOR_BGR2GRAY) if tileA.ndim == 3 else tileA.copy()\n",
        "    tBg = cv2.cvtColor(tileB, cv2.COLOR_BGR2GRAY) if tileB.ndim == 3 else tileB.copy()\n",
        "\n",
        "    # Ensure same size\n",
        "    if tAg.shape != tBg.shape:\n",
        "        tBg = cv2.resize(tBg, (tAg.shape[1], tAg.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    # SSIM: choose win_size <= min(dim) and odd and >=3 (skimage requires >=3, default 7)\n",
        "    min_side = min(tAg.shape[0], tAg.shape[1])\n",
        "    win_size = min(7, min_side)\n",
        "    if win_size < 3:\n",
        "        # fallback: if extremely small (shouldn't happen because we pad), just return neutral\n",
        "        ssim_val = 1.0\n",
        "    else:\n",
        "        if win_size % 2 == 0:\n",
        "            win_size -= 1\n",
        "        try:\n",
        "            ssim_val = ssim(tAg, tBg, win_size=win_size)\n",
        "        except Exception:\n",
        "            ssim_val = ssim(tAg, tBg, data_range=tAg.max()-tAg.min() if tAg.max()!=tAg.min() else 1)\n",
        "\n",
        "    # Intensity difference normalized [0,1]\n",
        "    absdiff = np.abs(tAg.astype(np.float32) - tBg.astype(np.float32))\n",
        "    mean_diff = absdiff.mean()  # 0..255\n",
        "    norm_diff = mean_diff / 255.0\n",
        "\n",
        "    return ssim_val, norm_diff\n",
        "\n",
        "# -----------------------------\n",
        "# Main pipeline\n",
        "# -----------------------------\n",
        "def run_tile_change_detection(sat_path, layout_path, tile_size=TILE_SIZE,\n",
        "                              hybrid_thresh=HYBRID_THRESH,\n",
        "                              ssim_w=SSIM_WEIGHT, int_w=INT_WEIGHT,\n",
        "                              out_dir=OUTPUT_DIR, save_tile_comparisons=SAVE_TILE_COMPARISONS):\n",
        "    ensure_dir(out_dir)\n",
        "    ensure_dir(os.path.join(out_dir, \"tiles_comparisons\"))\n",
        "\n",
        "    # Step 1: Load images\n",
        "    sat = load_image_bgr(sat_path)\n",
        "    layout = load_image_bgr(layout_path)\n",
        "    print(f\"Loaded sat: {sat.shape}, layout: {layout.shape}\")\n",
        "\n",
        "    # Step 2: Align by resizing layout to satellite shape (preserve sat resolution)\n",
        "    layout_resized = cv2.resize(layout, (sat.shape[1], sat.shape[0]), interpolation=cv2.INTER_AREA)\n",
        "    print(f\"Aligned layout to satellite size: {sat.shape}\")\n",
        "\n",
        "    # Save intermediate step visualizations\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step1_satellite.png\"), sat)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step1_layout_aligned.png\"), layout_resized)\n",
        "    print(\"Saved: step1_satellite.png, step1_layout_aligned.png\")\n",
        "\n",
        "    # Step 3: Pad images so dimensions divisible by tile_size (avoid tiny edge tiles)\n",
        "    sat_padded, (pad_h, pad_w) = pad_to_multiple(sat, tile_size)\n",
        "    layout_padded, _ = pad_to_multiple(layout_resized, tile_size)\n",
        "    print(f\"Padded images -> new shape: {sat_padded.shape} (pad_h={pad_h}, pad_w={pad_w})\")\n",
        "\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step2_satellite_padded.png\"), sat_padded)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step2_layout_padded.png\"), layout_padded)\n",
        "    print(\"Saved: step2_satellite_padded.png, step2_layout_padded.png\")\n",
        "\n",
        "    # Step 4: Iterate tiles and compute hybrid change score\n",
        "    coords = create_tile_coords(sat_padded.shape, tile_size)\n",
        "    heat_values = np.zeros((sat_padded.shape[0] // tile_size, sat_padded.shape[1] // tile_size), dtype=np.float32)\n",
        "    change_mask_tiles = np.zeros_like(heat_values, dtype=np.uint8)\n",
        "\n",
        "    comparisons_for_display = []  # store small set for display\n",
        "\n",
        "    for idx, (x, y) in enumerate(coords):\n",
        "        tx = x; ty = y\n",
        "        tileA = sat_padded[ty:ty+tile_size, tx:tx+tile_size]\n",
        "        tileB = layout_padded[ty:ty+tile_size, tx:tx+tile_size]\n",
        "\n",
        "        ssim_val, norm_diff = compute_tile_stats(tileA, tileB)\n",
        "\n",
        "        # hybrid score: higher => more change\n",
        "        hybrid_score = ssim_w * (1.0 - ssim_val) + int_w * norm_diff\n",
        "        # store into heat array\n",
        "        row = ty // tile_size\n",
        "        col = tx // tile_size\n",
        "        heat_values[row, col] = hybrid_score\n",
        "\n",
        "        # mark changed tile if hybrid_score > threshold\n",
        "        if hybrid_score > hybrid_thresh:\n",
        "            change_mask_tiles[row, col] = 1\n",
        "        else:\n",
        "            change_mask_tiles[row, col] = 0\n",
        "\n",
        "        # Save side-by-side comparison image for this tile (sat | layout | diff visualization)\n",
        "        if save_tile_comparisons:\n",
        "            # create visualization: sat (left), layout (mid), absdiff (right)\n",
        "            tA_rgb = tileA.copy()\n",
        "            tB_rgb = tileB.copy()\n",
        "            tA_gray = cv2.cvtColor(tA_rgb, cv2.COLOR_BGR2GRAY)\n",
        "            tB_gray = cv2.cvtColor(tB_rgb, cv2.COLOR_BGR2GRAY)\n",
        "            absd = cv2.absdiff(tA_gray, tB_gray)\n",
        "            # scale absd for display\n",
        "            absd_vis = cv2.cvtColor(absd, cv2.COLOR_GRAY2BGR)\n",
        "            # add a small colored overlay for hybrid score (red intensity)\n",
        "            red_overlay = np.zeros_like(tA_rgb)\n",
        "            red_int = int(np.clip(hybrid_score, 0, 1) * 255)\n",
        "            red_overlay[:, :, 2] = red_int\n",
        "            # blend overlay with actual tile for the rightmost small preview (optional)\n",
        "            blended = cv2.addWeighted(tA_rgb, 0.7, red_overlay, 0.3, 0)\n",
        "\n",
        "            comparison = np.hstack([tA_rgb, tB_rgb, absd_vis, blended])\n",
        "            fname = os.path.join(out_dir, \"tiles_comparisons\", f\"tile_{row}_{col}_hyb{hybrid_score:.3f}.png\")\n",
        "            cv2.imwrite(fname, comparison)\n",
        "\n",
        "            # keep a small set for inline display\n",
        "            if len(comparisons_for_display) < MAX_SAMPLE_TILES_DISPLAY:\n",
        "                comparisons_for_display.append((fname, hybrid_score, (row, col), ssim_val, norm_diff))\n",
        "\n",
        "    # Step 5: Build the pixel-level change heatmap (upsample tile heat to image size)\n",
        "    tile_rows, tile_cols = heat_values.shape\n",
        "    heatmap = cv2.resize(heat_values, (sat_padded.shape[1], sat_padded.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Normalize heatmap 0..1\n",
        "    heatmap_norm = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8)\n",
        "\n",
        "    # Create red overlay where heatmap intensity > 0 (we'll apply transparency)\n",
        "    overlay = sat_padded.copy().astype(np.float32)\n",
        "    red_layer = np.zeros_like(overlay)\n",
        "    red_layer[:, :, 2] = (heatmap_norm * 255).astype(np.uint8)  # red channel intensity proportional to heat\n",
        "\n",
        "    # binary mask of significant change by tile mask (expand tiles)\n",
        "    change_mask_pixels = cv2.resize(change_mask_tiles.astype(np.uint8),\n",
        "                                    (sat_padded.shape[1], sat_padded.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Create red-transparent overlay: alpha = heatmap_norm * change_mask_pixels\n",
        "    alpha = (heatmap_norm * change_mask_pixels).astype(np.float32)  # 0..1\n",
        "    alpha_3 = np.repeat(alpha[:, :, np.newaxis], 3, axis=2)\n",
        "\n",
        "    overlay_out = (overlay * (1 - alpha_3) + red_layer.astype(np.float32) * (alpha_3)).astype(np.uint8)\n",
        "\n",
        "    # Crop back to original size (remove padding) for saving/display\n",
        "    h0, w0 = sat.shape[:2]\n",
        "    overlay_cropped = overlay_out[0:h0, 0:w0]\n",
        "    heatmap_cropped = (heatmap_norm * 255).astype(np.uint8)[0:h0, 0:w0]\n",
        "    change_mask_cropped = change_mask_pixels[0:h0, 0:w0] * 255\n",
        "\n",
        "    # Step 6: Save and display results after major steps\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step3_tile_heat_raw.png\"), (heatmap_norm * 255).astype(np.uint8))\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step3_tile_binary_mask.png\"), change_mask_cropped)\n",
        "    cv2.imwrite(os.path.join(out_dir, \"final_overlay_on_satellite.png\"), overlay_cropped)\n",
        "\n",
        "    print(f\"\\nSaved outputs to: {out_dir}\")\n",
        "    print(\" - step3_tile_heat_raw.png (tile intensity heatmap, 0..255)\")\n",
        "    print(\" - step3_tile_binary_mask.png (binary changed tiles mask)\")\n",
        "    print(\" - final_overlay_on_satellite.png (satellite with red-transparent overlay)\")\n",
        "    # ---------------------------------------------------\n",
        "    # Step 7: Quantify percentage of area under deviation\n",
        "    # ---------------------------------------------------\n",
        "    print(\"\\nCalculating percentage of area under deviation...\")\n",
        "\n",
        "    # Use normalized heatmap and change mask for finer per-pixel deviation\n",
        "    deviation_threshold = hybrid_thresh  # same as your hybrid threshold\n",
        "    deviation_mask = (heatmap_norm > deviation_threshold).astype(np.uint8)\n",
        "\n",
        "    # Compute area percentage\n",
        "    changed_pixels = np.sum(deviation_mask)\n",
        "    total_pixels = deviation_mask.size\n",
        "    deviation_percent_area = (changed_pixels / total_pixels) * 100.0\n",
        "\n",
        "    print(f\"ðŸ“Š Estimated deviation area: {deviation_percent_area:.2f}% of total region.\")\n",
        "\n",
        "    # Optional overlay visualization (red = deviation) â€“ fixed for shape/channel mismatch\n",
        "    heat_colored = cv2.applyColorMap((deviation_mask * 255).astype(np.uint8), cv2.COLORMAP_HOT)\n",
        "    heat_colored = cv2.resize(heat_colored, (sat.shape[1], sat.shape[0]))  # match satellite size\n",
        "\n",
        "    # ensure both have 3 channels\n",
        "    if len(sat.shape) == 2:\n",
        "        sat_color = cv2.cvtColor(sat, cv2.COLOR_GRAY2BGR)\n",
        "    else:\n",
        "        sat_color = sat.copy()\n",
        "\n",
        "    # Blend red heatmap overlay (red = deviation)\n",
        "    overlay_dev = cv2.addWeighted(sat_color, 0.7, heat_colored, 0.3, 0)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(cv2.cvtColor(overlay_dev, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(f\"Deviation Area Overlay ({deviation_percent_area:.2f}% area changed)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save overlay for reference\n",
        "    cv2.imwrite(os.path.join(out_dir, \"step4_deviation_area_overlay.png\"), overlay_dev)\n",
        "    print(\"Saved: step4_deviation_area_overlay.png (Deviation heat overlay)\")\n",
        "\n",
        "\n",
        "    # Inline displays\n",
        "    plt.figure(figsize=(14,6))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(cv2.cvtColor(sat, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Original Satellite\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(cv2.cvtColor(layout_resized, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Aligned Layout\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(cv2.cvtColor(overlay_cropped, cv2.COLOR_BGR2RGB))\n",
        "    plt.title(\"Overlay (Red = Deviation)\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Show heatmap separately (red intensity)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(heatmap_cropped, cmap=\"hot\")\n",
        "    plt.title(\"Tile Hybrid Heatmap (hot)\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "    # Show some tile comparisons\n",
        "    if comparisons_for_display:\n",
        "        print(\"\\nSample tile comparisons saved (sat | layout | absdiff | blended overlay):\")\n",
        "        for fname, hybrid_score, (r,c), ssim_val, norm_diff in comparisons_for_display:\n",
        "            print(f\" Tile ({r},{c}): hybrid={hybrid_score:.3f}, ssim={ssim_val:.3f}, int_diff={norm_diff:.3f}  -> {fname}\")\n",
        "            comp = Image.open(fname)\n",
        "            display(comp)  # in notebooks this will show the image\n",
        "    else:\n",
        "        print(\"No tile comparisons to display.\")\n",
        "\n",
        "    # Summary stats\n",
        "    total_tiles = tile_rows * tile_cols\n",
        "    changed_tiles = int(change_mask_tiles.sum())\n",
        "    percent_changed_tiles = (changed_tiles / total_tiles) * 100.0\n",
        "    mean_hybrid = float(heat_values.mean())\n",
        "\n",
        "    print(\"\\n=== SUMMARY ===\")\n",
        "    print(f\"Tile size: {tile_size} px\")\n",
        "    print(f\"Tiles: {total_tiles} (rows={tile_rows}, cols={tile_cols})\")\n",
        "    print(f\"Changed tiles: {changed_tiles}\")\n",
        "    print(f\"Change coverage (tiles): {percent_changed_tiles:.2f}%\")\n",
        "    print(f\"Mean hybrid score: {mean_hybrid:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"heat_values\": heat_values,\n",
        "        \"change_mask_tiles\": change_mask_tiles,\n",
        "        \"overlay_path\": os.path.join(out_dir, \"final_overlay_on_satellite.png\"),\n",
        "        \"heatmap_path\": os.path.join(out_dir, \"step3_tile_heat_raw.png\"),\n",
        "        \"tile_comparisons_dir\": os.path.join(out_dir, \"tiles_comparisons\")\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# Run (interactive tile size)\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ensure_dir(OUTPUT_DIR)\n",
        "    try:\n",
        "        ts_input = input(f\"Enter tile size (default {TILE_SIZE}): \").strip()\n",
        "        if ts_input != \"\":\n",
        "            TILE_SIZE = int(ts_input)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Use medium sensitivity (threshold=0.3) as requested\n",
        "    res = run_tile_change_detection(SATELLITE_PATH, LAYOUT_PATH, tile_size=TILE_SIZE,\n",
        "                                    hybrid_thresh=HYBRID_THRESH)\n"
      ],
      "metadata": {
        "id": "rS-NHMv6fadj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segmentation\n",
        "\n",
        "Goal: Want to achieve clear road & wall bundaries on grayscale."
      ],
      "metadata": {
        "id": "biieYTupe-on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 1\n",
        "### Applying K means and Gray scale segementation\n",
        "\n",
        "Link: https://colab.research.google.com/drive/11gVZr-GnOizhG8OowdTCLfaaOLKD-Ish#scrollTo=xD2OlEsxyQmT\n",
        "\n",
        "Outcome: Good result on Map layouts, but algorithm capture noise in real satellite images (like trees, bushes, vehicles)"
      ],
      "metadata": {
        "id": "Xf7SxCw6ADNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image extractor from pdf and applying K means and Gray scale segementation"
      ],
      "metadata": {
        "id": "lQUKwKEFCV7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pdf2image import convert_from_path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from PIL import Image\n",
        "\n",
        "# ------------------------\n",
        "# Configurable parameters\n",
        "# ------------------------\n",
        "KMEANS_SAMPLE_PIXELS = 500_000   # sample for training only (output stays full resolution)\n",
        "N_CLUSTERS = 5\n",
        "KMEANS_N_INIT = 10\n",
        "CANNY_THRESHOLDS = (100, 200)\n",
        "\n",
        "# ------------------------\n",
        "# Helpers\n",
        "# ------------------------\n",
        "def convert_pdf_to_images(pdf_file_path, dpi=300):\n",
        "    \"\"\"Convert PDF pages to PIL images.\"\"\"\n",
        "    try:\n",
        "        pages = convert_from_path(pdf_file_path, dpi=dpi)\n",
        "        print(f\"Converted PDF to {len(pages)} pages at {dpi} dpi.\")\n",
        "        return pages\n",
        "    except Exception as e:\n",
        "        print(\"Error converting PDF:\", e)\n",
        "        return None\n",
        "\n",
        "def ensure_dir(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def save_images_to_pdf_pillow(image_paths, output_pdf=\"output.pdf\"):\n",
        "    \"\"\"Save images as a multi-page PDF without rescaling (lossless).\"\"\"\n",
        "    if not image_paths:\n",
        "        print(\"No images to save as PDF.\")\n",
        "        return None\n",
        "    pil_images = [Image.open(p).convert(\"RGB\") for p in image_paths]\n",
        "    first, rest = pil_images[0], pil_images[1:]\n",
        "    first.save(output_pdf, save_all=True, append_images=rest)\n",
        "    print(f\"PDF saved with {len(pil_images)} pages: {output_pdf}\")\n",
        "    return output_pdf\n",
        "\n",
        "# ------------------------\n",
        "# 1) Color segmentation\n",
        "# ------------------------\n",
        "def color_segmentation_kmeans(input_pdf, output_dir=\"color_segmented\"):\n",
        "    ensure_dir(output_dir)\n",
        "    pil_pages = convert_pdf_to_images(input_pdf)\n",
        "    if not pil_pages:\n",
        "        return []\n",
        "\n",
        "    saved_paths = []\n",
        "    for idx, pil in enumerate(pil_pages, start=1):\n",
        "        print(f\"\\n[Color KMeans] Page {idx}...\")\n",
        "        cv_img = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        h, w = cv_img.shape[:2]\n",
        "        flat = cv_img.reshape((-1, 3)).astype(np.float32)\n",
        "\n",
        "        # Sample subset for clustering (for speed/memory only)\n",
        "        n_pixels = flat.shape[0]\n",
        "        if n_pixels > KMEANS_SAMPLE_PIXELS:\n",
        "            sample_idx = np.random.choice(n_pixels, size=KMEANS_SAMPLE_PIXELS, replace=False)\n",
        "            sample_flat = flat[sample_idx]\n",
        "        else:\n",
        "            sample_flat = flat\n",
        "\n",
        "        kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, n_init=KMEANS_N_INIT, random_state=42)\n",
        "        kmeans.fit(sample_flat)\n",
        "\n",
        "        labels = kmeans.predict(flat)\n",
        "        segmented_flat = kmeans.cluster_centers_[labels]\n",
        "        segmented = segmented_flat.reshape((h, w, 3)).astype(np.uint8)\n",
        "\n",
        "        out_path = os.path.join(output_dir, f\"segmented_page_{idx}.png\")\n",
        "        cv2.imwrite(out_path, segmented)\n",
        "        saved_paths.append(out_path)\n",
        "        print(f\"Saved segmented PNG: {out_path}\")\n",
        "\n",
        "        plt.imshow(cv2.cvtColor(segmented, cv2.COLOR_BGR2RGB))\n",
        "        plt.title(f\"Segmented Page {idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return saved_paths\n",
        "\n",
        "# ------------------------\n",
        "# 2) Grayscale + sharpen + edges\n",
        "# ------------------------\n",
        "def grayscale_sharpen_edges(image_paths, output_dir=\"grayscale_edges\"):\n",
        "    ensure_dir(output_dir)\n",
        "    if not image_paths:\n",
        "        print(\"No images to process.\")\n",
        "        return []\n",
        "\n",
        "    saved = []\n",
        "    for idx, p in enumerate(image_paths, start=1):\n",
        "        print(f\"\\n[Grayscale Enhanced] Page {idx} - {p}\")\n",
        "        img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            print(f\"Failed to read {p}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Step 1: Sharpen (optional but helps edges pop)\n",
        "        sharpening_kernel = np.array([[-1, -1, -1],\n",
        "                                      [-1,  9, -1],\n",
        "                                      [-1, -1, -1]])\n",
        "        sharpened = cv2.filter2D(img, -1, sharpening_kernel)\n",
        "\n",
        "        # Step 2: Convert to grayscale\n",
        "        gray = cv2.cvtColor(sharpened, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Step 3: Denoise while keeping edges\n",
        "        denoised = cv2.bilateralFilter(gray, d=9, sigmaColor=75, sigmaSpace=75)\n",
        "\n",
        "        # Step 4: Contrast enhancement (CLAHE)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        enhanced = clahe.apply(denoised)\n",
        "\n",
        "        # Step 5: Auto Canny thresholds\n",
        "        median_val = np.median(enhanced)\n",
        "        low = int(max(0, 0.66 * median_val))\n",
        "        high = int(min(255, 1.33 * median_val))\n",
        "\n",
        "        print(f\"Auto Canny thresholds â†’ low={low}, high={high}\")\n",
        "        edges = cv2.Canny(enhanced, low, high)\n",
        "\n",
        "        # Save output\n",
        "        out_path = os.path.join(output_dir, f\"edges_page_{idx}.png\")\n",
        "        cv2.imwrite(out_path, edges)\n",
        "        saved.append(out_path)\n",
        "        print(f\"Saved enhanced edge image: {out_path}\")\n",
        "\n",
        "        # Show preview\n",
        "        plt.imshow(edges, cmap='gray')\n",
        "        plt.title(f\"Enhanced Edges Page {idx}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    return saved\n",
        "\n",
        "# ------------------------\n",
        "# Combined pipeline\n",
        "# ------------------------\n",
        "def process_pdf_pipeline(input_pdf,\n",
        "                         color_out_dir=\"color_segmented\",\n",
        "                         edges_out_dir=\"grayscale_edges\",\n",
        "                         color_pdf=\"color_segmented_output.pdf\",\n",
        "                         edges_pdf=\"grayscale_edges_output.pdf\"):\n",
        "    print(\"STEP 1: Color segmentation\")\n",
        "    segmented_paths = color_segmentation_kmeans(input_pdf, output_dir=color_out_dir)\n",
        "\n",
        "    print(\"\\nSTEP 2: Grayscale + edges\")\n",
        "    edge_paths = grayscale_sharpen_edges(segmented_paths, output_dir=edges_out_dir)\n",
        "\n",
        "    print(\"\\nSTEP 3: Save outputs as PDFs (lossless, no resizing)\")\n",
        "    if segmented_paths:\n",
        "        save_images_to_pdf_pillow(segmented_paths, output_pdf=color_pdf)\n",
        "    if edge_paths:\n",
        "        save_images_to_pdf_pillow(edge_paths, output_pdf=edges_pdf)\n",
        "\n",
        "    print(\"\\nPipeline completed.\")\n",
        "    return segmented_paths, edge_paths, (color_pdf, edges_pdf)\n",
        "\n",
        "# ------------------------\n",
        "# Example usage\n",
        "# ------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    input_pdf = \"/content/map_no_selectable_text.pdf\"\n",
        "    process_pdf_pipeline(input_pdf)"
      ],
      "metadata": {
        "id": "RCEGh1trACxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single image Segementation (If you don't working on pdf)"
      ],
      "metadata": {
        "id": "xI8rcQWIB78c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cell 1: Setup and Imports\n",
        "\n",
        "# Before running this code, you need to install the necessary libraries.\n",
        "# You can do this by running the following commands in your terminal or a Jupyter cell:\n",
        "# pip install opencv-python Pillow numpy matplotlib\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# The path to the satellite image file you want to process.\n",
        "# Please replace 'satellite_image.jpg' with the actual path to your file.\n",
        "# Make sure the image file is in the same directory as this notebook, or provide the full path.\n",
        "image_path = '/content/segmented_image_page_8.png'\n",
        "\n",
        "### Cell 2: Load and Display Original Image\n",
        "\n",
        "# This section loads the satellite image from the file system and displays it.\n",
        "def load_image(image_file_path):\n",
        "    \"\"\"\n",
        "    Loads an image from the specified file path.\n",
        "\n",
        "    Args:\n",
        "        image_file_path (str): The path to the image file.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: The image as a NumPy array (OpenCV format), or None if an error occurs.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_file_path):\n",
        "        print(f\"Error: The file '{image_file_path}' does not exist.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load the image using OpenCV\n",
        "        img = cv2.imread(image_file_path)\n",
        "        if img is None:\n",
        "            print(\"Error: Could not read the image file. Check file format and permissions.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Successfully loaded image from '{image_file_path}'.\")\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the image: {e}\")\n",
        "        return None\n",
        "\n",
        "# Load the satellite image\n",
        "satellite_image = load_image(image_path)\n",
        "\n",
        "if satellite_image is not None:\n",
        "    # Display the original image\n",
        "    plt.imshow(cv2.cvtColor(satellite_image, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Satellite Image')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "### Cell 3: Apply Edge Detection\n",
        "\n",
        "# This section applies the Canny edge detection algorithm, a popular technique\n",
        "# for finding sharp changes in image intensity, which correspond to edges.\n",
        "# This is a form of image segmentation where the segments are defined by their boundaries.\n",
        "\n",
        "if satellite_image is not None:\n",
        "    # Convert the image to grayscale, as Canny edge detection works on single-channel images.\n",
        "    gray_image = cv2.cvtColor(satellite_image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Canny edge detection\n",
        "    # The thresholds control which edges are detected.\n",
        "    # A low threshold of 100 and a high threshold of 200 are common starting points.\n",
        "    low_threshold = 100\n",
        "    high_threshold = 200\n",
        "\n",
        "    print(f\"\\nApplying Canny edge detection with thresholds: low={low_threshold}, high={high_threshold}...\")\n",
        "    edges = cv2.Canny(gray_image, low_threshold, high_threshold)\n",
        "\n",
        "    # Display the resulting edges\n",
        "    plt.imshow(edges, cmap='gray')\n",
        "    plt.title('Canny Edge Detection Result')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Save the resulting image\n",
        "    output_path = 'edges.png'\n",
        "    cv2.imwrite(output_path, edges)\n",
        "    print(f\"\\nEdge-detected image saved as '{output_path}'\")\n",
        "else:\n",
        "    print(\"Could not find the image to perform edge detection. Please check the previous cell.\")\n"
      ],
      "metadata": {
        "id": "B5GmMKCYB6Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "npap8CxtB6B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 2\n",
        "Applied SSIM directly on Satellite image and layout directly\n",
        "\n",
        "Link: https://colab.research.google.com/drive/1uxKlL402nxPk1Se2jkFoEcyIM0MMzW6g#scrollTo=rp6I0BnjDGPy\n",
        "\n",
        "Outcome: Better results, but same noise issue"
      ],
      "metadata": {
        "id": "KZnoA8p0EcZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aprroach 3\n",
        "### Trained Unet model on \"massachusetts-roads-dataset\"\n",
        "Link: https://colab.research.google.com/drive/1uYOffbsvZ_2j7SpfCBEv6sdGg-X8JSNk\n",
        "\n",
        "Outcome: Model fail on local aligarh maps"
      ],
      "metadata": {
        "id": "RGR-3q4-iXG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approach 4\n",
        "Created Hand labels from Satellite images and then trained the Unet model on Alig Dataset\n",
        "\n",
        "Compared Layouts with Unet Model Output (Satellite images segmentations)...\n",
        "\n",
        "Model Traing available on the following link\n",
        "\n",
        "https://colab.research.google.com/drive/1i4Mg1hItzSevyb_l0D3BsrV2MZYnXnqw\n",
        "\n",
        "Outcome: Model failure, due to less data item (53 images) model learns only black pixels"
      ],
      "metadata": {
        "id": "YfRnzD6fD9g4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Approch 5\n",
        "### Transfer Learning\n",
        "\n",
        "Fine Tune Cuda's ResNet model on our own Alig dataset\n",
        "\n",
        "Link: https://colab.research.google.com/drive/1i4Mg1hItzSevyb_l0D3BsrV2MZYnXnqw\n",
        "\n",
        "Outcome: Best results till now..."
      ],
      "metadata": {
        "id": "3s8HYdutj6gL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best result when training for 50 epochs"
      ],
      "metadata": {
        "id": "bDx2QGjwxpcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================================\n",
        "# 1. SETUP, IMPORTS, AND PATHS\n",
        "# =========================================================================\n",
        "\n",
        "# Install necessary libraries silently\n",
        "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
        "!pip install scikit-image --upgrade\n",
        "\n",
        "import os, cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random, tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations as album\n",
        "import segmentation_models_pytorch as smp\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"Libraries loaded successfully.\")\n",
        "\n",
        "# --- Define Paths and Constants ---\n",
        "# Assuming 'Alig_Dataset' was uploaded directly to the Colab working directory.\n",
        "# NOTE: The mask folder name is assumed to be 'Label' based on the last input. Change to 'Label_filled' if applicable.\n",
        "LOCAL_PATH = './Alig_Dataset/'\n",
        "x_train_dir = os.path.join(LOCAL_PATH, 'Satellite_img')\n",
        "y_train_dir = os.path.join(LOCAL_PATH, 'Label')\n",
        "x_valid_dir = os.path.join(LOCAL_PATH, 'Satellite_img')\n",
        "y_valid_dir = os.path.join(LOCAL_PATH, 'Label')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# =========================================================================\n",
        "# 2. HELPER FUNCTIONS, CLASSES, AND U-NET ARCHITECTURE (TRANSFER LEARNING)\n",
        "# =========================================================================\n",
        "\n",
        "# --- Define 2-Class Binary Segmentation ---\n",
        "class_names = ['background', 'building_outline']\n",
        "class_rgb_values = [\n",
        "    [0, 0, 0],      # Black for Background\n",
        "    [255, 255, 255] # White for Building Outline\n",
        "]\n",
        "select_classes = ['background', 'building_outline']\n",
        "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
        "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
        "\n",
        "# --- Visualization and Encoding Functions ---\n",
        "def visualize(**images):\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); plt.yticks([])\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        if image.ndim == 2 or (image.ndim == 3 and image.shape[2] == 1):\n",
        "            plt.imshow(image.squeeze(), cmap='gray')\n",
        "        else:\n",
        "            plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "def one_hot_encode(label, label_values):\n",
        "    semantic_map = []\n",
        "    for colour in label_values:\n",
        "        equality = np.equal(label, colour)\n",
        "        class_map = np.all(equality, axis = -1)\n",
        "        semantic_map.append(class_map)\n",
        "    semantic_map = np.stack(semantic_map, axis=-1)\n",
        "    return semantic_map.astype('float')\n",
        "\n",
        "def reverse_one_hot(image):\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "    return x\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2, 0, 1).astype('float32')\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):\n",
        "    _transform = []\n",
        "    if preprocessing_fn:\n",
        "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
        "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
        "    return album.Compose(_transform)\n",
        "\n",
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
        "        album.OneOf([album.HorizontalFlip(p=1), album.VerticalFlip(p=1), album.RandomRotate90(p=1)], p=0.75),\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    test_transform = [album.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),]\n",
        "    return album.Compose(test_transform)\n",
        "\n",
        "\n",
        "# --- U-Net Model Initialization (TRANSFER LEARNING FIX) ---\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",       # Use ResNet34 encoder\n",
        "    encoder_weights=\"imagenet\",    # Load weights pre-trained on ImageNet\n",
        "    in_channels=3,                 # RGB input\n",
        "    classes=len(select_classes),   # 2 output classes (Background, Building)\n",
        ")\n",
        "model.to(DEVICE)\n",
        "print(f\"U-Net Model (ResNet34, ImageNet pre-trained) initialized.\")\n",
        "\n",
        "\n",
        "# =========================================================================\n",
        "# 3. CUSTOM DATASET AND DATALOADERS\n",
        "# =========================================================================\n",
        "\n",
        "class AligarhDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, images_dir, masks_dir, is_train=True, augmentation=None, preprocessing=None, class_rgb_values=None):\n",
        "\n",
        "        # Diagnostics: Checks existence of files\n",
        "        try:\n",
        "            satellite_files = sorted(os.listdir(images_dir))\n",
        "            file_ids = sorted([f[1:-4] for f in satellite_files if f.startswith('S') and f.endswith('.png')])\n",
        "        except FileNotFoundError:\n",
        "            file_ids = []\n",
        "\n",
        "        total_files_found = len(file_ids)\n",
        "        if total_files_found == 0:\n",
        "            self.image_paths = []; self.mask_paths = []; return\n",
        "\n",
        "        split_idx = int(total_files_found * 0.8)\n",
        "\n",
        "        if is_train:\n",
        "            self.file_ids = file_ids[:split_idx]\n",
        "        else:\n",
        "            self.file_ids = file_ids[split_idx:]\n",
        "\n",
        "        self.image_paths = [os.path.join(images_dir, f'S{id_}.png') for id_ in self.file_ids]\n",
        "        self.mask_paths = [os.path.join(masks_dir, f'M{id_}.png') for id_ in self.file_ids]\n",
        "\n",
        "        self.class_rgb_values = class_rgb_values\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # NOTE: num_workers=0 is assumed in DataLoader to handle cv2.imread safely\n",
        "        image_bgr = cv2.imread(self.image_paths[i])\n",
        "        mask_bgr = cv2.imread(self.mask_paths[i])\n",
        "\n",
        "        if image_bgr is None or mask_bgr is None:\n",
        "             raise RuntimeError(f\"Failed to load image or mask: {self.image_paths[i]}\")\n",
        "\n",
        "        image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(mask_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
        "\n",
        "        if self.augmentation:\n",
        "            sample = self.augmentation(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        if self.preprocessing:\n",
        "            sample = self.preprocessing(image=image, mask=mask)\n",
        "            image, mask = sample['image'], sample['mask']\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)\n",
        "\n",
        "# --- Initialize DataLoaders ---\n",
        "train_dataset = AligarhDataset(x_train_dir, y_train_dir, is_train=True, augmentation=get_training_augmentation(), preprocessing=get_preprocessing(preprocessing_fn=None), class_rgb_values=select_class_rgb_values)\n",
        "valid_dataset = AligarhDataset(x_valid_dir, y_valid_dir, is_train=False, augmentation=get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn=None), class_rgb_values=select_class_rgb_values)\n",
        "\n",
        "# num_workers=0 is set for stability with cv2.imread\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nDataLoaders initialized (Train: {len(train_dataset)}, Valid: {len(valid_dataset)}).\")\n",
        "\n",
        "\n",
        "# =========================================================================\n",
        "# 4. TRAINING AND VALIDATION EXECUTION\n",
        "# =========================================================================\n",
        "\n",
        "TRAINING = True\n",
        "EPOCHS = 50 # Increased epochs for stability and learning with small data\n",
        "loss = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam([dict(params=model.parameters(), lr=0.00008)])\n",
        "\n",
        "# --- Training and Validation Loop Classes (Standard PyTorch) ---\n",
        "class CustomTrainEpoch:\n",
        "    def __init__(self, model, loss, optimizer, device):\n",
        "        self.model = model; self.loss = loss; self.optimizer = optimizer; self.device = device\n",
        "\n",
        "    def run(self, data_loader):\n",
        "        self.model.train(); total_loss = 0\n",
        "        pbar = tqdm.tqdm(data_loader)\n",
        "        for x_batch, y_batch in pbar:\n",
        "            x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "            y_pred_logits = self.model(x_batch)\n",
        "            loss_value = self.loss(y_pred_logits, y_batch)\n",
        "            loss_value.backward(); self.optimizer.step()\n",
        "            total_loss += loss_value.item()\n",
        "            pbar.set_description(f\"Loss: {loss_value.item():.4f}\")\n",
        "        return {'loss': total_loss / len(data_loader)}\n",
        "\n",
        "class CustomValidEpoch(CustomTrainEpoch):\n",
        "    def run(self, data_loader):\n",
        "        self.model.eval(); total_loss = 0; total_iou = 0\n",
        "        pbar = tqdm.tqdm(data_loader)\n",
        "        with torch.no_grad():\n",
        "            for x_batch, y_batch in pbar:\n",
        "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
        "                y_pred_logits = self.model(x_batch)\n",
        "                loss_value = self.loss(y_pred_logits, y_batch); total_loss += loss_value.item()\n",
        "\n",
        "                y_pred_prob = torch.sigmoid(y_pred_logits); y_pred_mask = (y_pred_prob > 0.5).float()\n",
        "\n",
        "                intersection = (y_pred_mask * y_batch).sum()\n",
        "                union = y_pred_mask.sum() + y_batch.sum() - intersection\n",
        "                iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "                total_iou += iou.item()\n",
        "\n",
        "                pbar.set_description(f\"Val Loss: {loss_value.item():.4f}, IoU: {iou.item():.4f}\")\n",
        "\n",
        "        return {'loss': total_loss / len(data_loader), 'iou_score': total_iou / len(data_loader)}\n",
        "\n",
        "# --- Training Loop Execution ---\n",
        "if TRAINING:\n",
        "    best_iou_score = 0.0\n",
        "    for i in range(0, EPOCHS):\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        CustomTrainEpoch(model, loss, optimizer, DEVICE).run(train_loader)\n",
        "        valid_logs = CustomValidEpoch(model, loss, optimizer, DEVICE).run(valid_loader)\n",
        "\n",
        "        if valid_logs['iou_score'] > best_iou_score:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model.state_dict(), './best_model.pth')\n",
        "            print('Model saved!')\n",
        "\n",
        "\n",
        "# =========================================================================\n",
        "# 5. PREDICTION AND ACCURACY ASSESSMENT (IoU)\n",
        "# =========================================================================\n",
        "\n",
        "# --- 1. Load Trained Model ---\n",
        "try:\n",
        "    best_model = smp.Unet(encoder_name=\"resnet34\", in_channels=3, classes=len(select_classes))\n",
        "    state_dict = torch.load('./best_model.pth', map_location=DEVICE)\n",
        "    best_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    best_model.to(DEVICE); best_model.eval()\n",
        "    print('\\nLoaded trained U-Net model successfully.')\n",
        "except Exception as e:\n",
        "    print(f\"[CRITICAL ERROR] Model loading failed. Ensure training finished successfully. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Prediction Helper ---\n",
        "def predict_image(model, image, device, target_size=(512, 512)):\n",
        "    image_tensor = get_preprocessing()(image=image)['image']\n",
        "    x_tensor = torch.from_numpy(image_tensor).to(device).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_logits = model(x_tensor)\n",
        "\n",
        "    pred_prob = torch.sigmoid(pred_logits).squeeze().cpu().numpy()\n",
        "    predicted_mask_binary = (pred_prob[1, :, :] > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "    return cv2.resize(predicted_mask_binary, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "def calculate_iou_for_single_image(pred_mask_binary, gt_mask_path, target_size=(512, 512)):\n",
        "    \"\"\"Calculates Intersection over Union (IoU) between the predicted mask and the ground truth mask.\"\"\"\n",
        "    gt_mask_raw = cv2.cvtColor(cv2.imread(gt_mask_path), cv2.COLOR_BGR2RGB)\n",
        "    gt_mask_gray = cv2.cvtColor(gt_mask_raw, cv2.COLOR_RGB2GRAY)\n",
        "    gt_mask_resized = cv2.resize(gt_mask_gray, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    gt_tensor = torch.from_numpy(gt_mask_resized).float() / 255.0\n",
        "    pred_tensor = torch.from_numpy(pred_mask_binary).float() / 255.0\n",
        "\n",
        "    intersection = (pred_tensor * gt_tensor).sum()\n",
        "    union = pred_tensor.sum() + gt_tensor.sum() - intersection\n",
        "\n",
        "    iou = (intersection.item() + 1e-6) / (union.item() + 1e-6)\n",
        "    return iou\n",
        "\n",
        "\n",
        "# --- 3. EXECUTION: Predict and Compare against Ground Truth ---\n",
        "TARGET_SIZE = (512, 512)\n",
        "S_PATH = os.path.join(x_train_dir, 'S05.png')\n",
        "M_PATH = os.path.join(y_train_dir, 'M05.png')\n",
        "\n",
        "# Load Input Image\n",
        "satellite_image_raw = cv2.cvtColor(cv2.imread(S_PATH), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Generate the Actual Segmentation Mask (U-Net Output)\n",
        "predicted_mask_binary = predict_image(best_model, satellite_image_raw, DEVICE, target_size=TARGET_SIZE)\n",
        "\n",
        "# Calculate IoU Score\n",
        "iou_score = calculate_iou_for_single_image(predicted_mask_binary, M_PATH, TARGET_SIZE)\n",
        "\n",
        "# Load Ground Truth Mask for Visualization\n",
        "ground_truth_mask = cv2.cvtColor(cv2.imread(M_PATH), cv2.COLOR_BGR2RGB)\n",
        "ground_truth_mask_resized = cv2.resize(cv2.cvtColor(ground_truth_mask, cv2.COLOR_RGB2GRAY), TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "\n",
        "print(f\"\\n=======================================================\")\n",
        "print(f\"âœ… U-Net Prediction vs. Ground Truth (M05.png) IoU Score: {iou_score:.4f}\")\n",
        "print(f\"=======================================================\")\n",
        "\n",
        "visualize(\n",
        "    Satellite_Input = satellite_image_raw,\n",
        "    Ground_Truth_Mask = ground_truth_mask_resized,\n",
        "    Predicted_Mask_UNet = predicted_mask_binary\n",
        ")"
      ],
      "metadata": {
        "id": "JC3kg72Jx6Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate segmentated image for input satellite image using best_model.pth"
      ],
      "metadata": {
        "id": "LUq4hP52yJ2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================================================\n",
        "# 6. DEPLOYMENT: PREDICT ON UNSEEN USER IMAGES AND SAVE MASK\n",
        "# =========================================================================\n",
        "\n",
        "import os, cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import segmentation_models_pytorch as smp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- 1. Load Trained Model (Setup) ---\n",
        "try:\n",
        "    # Model architecture must match the trained model (ResNet34, 2 classes)\n",
        "    best_model = smp.Unet(encoder_name=\"resnet34\", in_channels=3, classes=2)\n",
        "    state_dict = torch.load('./best_model.pth', map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    best_model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_model.to(DEVICE);\n",
        "    best_model.eval()\n",
        "    print('Deployment model loaded successfully.')\n",
        "except Exception as e:\n",
        "    print(f\"[CRITICAL ERROR] Failed to load model for deployment. Ensure training was successful. Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Prediction Function (Includes Saving) ---\n",
        "def predict_and_visualize_user_image(image_path, model, device, save_filename, target_size=(512, 512)):\n",
        "\n",
        "    # Load and preprocess the user image\n",
        "    image_bgr = cv2.imread(image_path)\n",
        "    if image_bgr is None:\n",
        "        print(f\"ERROR: Could not find image at path: {image_path}\")\n",
        "        return\n",
        "\n",
        "    original_image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Preprocessing\n",
        "    # NOTE: Assuming the 'get_preprocessing' function from the previous code is available\n",
        "    image_tensor = get_preprocessing()(image=original_image_rgb)['image']\n",
        "    x_tensor = torch.from_numpy(image_tensor).to(device).unsqueeze(0)\n",
        "\n",
        "    # Run Prediction\n",
        "    with torch.no_grad():\n",
        "        pred_logits = model(x_tensor)\n",
        "\n",
        "    pred_prob = torch.sigmoid(pred_logits).squeeze().cpu().numpy()\n",
        "    predicted_mask_binary = (pred_prob[1, :, :] > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "    # --- Morphological Closing Fix: Fills lines/gaps into solid shapes ---\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    predicted_mask_binary = cv2.morphologyEx(predicted_mask_binary, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    predicted_mask_resized = cv2.resize(predicted_mask_binary, target_size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # --- CRITICAL ACTION: SAVE THE MASK ---\n",
        "    cv2.imwrite(save_filename, predicted_mask_resized)\n",
        "    print(f\"âœ… Generated Segmentation Mask successfully saved as: {save_filename}\")\n",
        "\n",
        "    # --- Visualization ---\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original_image_rgb)\n",
        "    plt.title(f\"Input Image: {os.path.basename(image_path)}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(predicted_mask_resized, cmap='gray')\n",
        "    plt.title(\"Generated Segmentation Mask\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "    return predicted_mask_resized\n",
        "\n",
        "# =========================================================================\n",
        "# 3. USER INTERFACE (Test and Save)\n",
        "# =========================================================================\n",
        "\n",
        "# ===> CHANGE THIS ID TO TEST ANY IMAGE (e.g., 'S15' for S15.png) <===\n",
        "TEST_IMAGE_ID = '1'\n",
        "USER_INPUT_IMAGE_PATH = '/content/SV2_Etah.png'\n",
        "\n",
        "# ===> DEFINE THE OUTPUT FILENAME HERE <===\n",
        "OUTPUT_FILENAME = f\"segmented_mask_{TEST_IMAGE_ID}.png\"\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Execute the prediction function\n",
        "# ----------------------------------------------------\n",
        "final_mask = predict_and_visualize_user_image(USER_INPUT_IMAGE_PATH, best_model, DEVICE, OUTPUT_FILENAME)"
      ],
      "metadata": {
        "id": "ks4KY6mAyXwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "### Test run on different possibilites\n",
        "ResNet on\n",
        "\n",
        "With Road\n",
        "\n",
        "Without Road\n",
        "\n",
        "Link: https://colab.research.google.com/drive/1twvb8eTJhAsOl9ndvcDifQF_ra8KobMV\n",
        "\n",
        "Massachusetts-road-dataset:\n",
        "\n",
        "...To be continue..."
      ],
      "metadata": {
        "id": "zr9wVNcltgfM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ri6R1L5fACuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kYtGEfJ_8Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcwflhUN_7-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MyeXq3mC_779"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjhWR4e5_76A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKiEia0w_73l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}